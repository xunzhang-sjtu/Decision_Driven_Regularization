{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import YahooNewsDataExtraction as tool\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 50\n",
    "train_val_split = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training set Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Read in train data'''\n",
    "'''Col0 - Col4 are features, Col5 is adID, Col6 is click binary indicator.'''\n",
    "train_records = np.vstack([np.load('day' + str(day) + '_records.npy') for day in range(1,6)]) \n",
    "\n",
    "\n",
    "'''Read in ads dictionary.'''\n",
    "train_adsDict = dict()\n",
    "for day in range(1,6):\n",
    "    train_adsDict.update(pickle.load(open('day' + str(day) + '_adsDict.p','rb')))\n",
    "\n",
    "    \n",
    "'''Add user type to each user interaction record.'''\n",
    "filename = 'train_userCluster.p'\n",
    "user_kmeans = pickle.load(open(filename,'rb'))\n",
    "train_recordsDF = pd.DataFrame(np.hstack([train_records,user_kmeans.labels_.reshape(-1,1)])).rename(columns \n",
    "                                                                                                    ={5:'adID', 6: 'click',7: 'userType'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Cluster articles into different types.'''\n",
    "train_adsDF = pd.DataFrame(train_adsDict).T\n",
    "n_adsClusters = 7\n",
    "ads_kmeans = KMeans(n_clusters=n_adsClusters, random_state=300)\n",
    "ads_kmeans.fit(train_adsDF)\n",
    "ads_kmeans.labels_\n",
    "train_adsType = dict(zip(train_adsDF.dropna().index, ads_kmeans.labels_))\n",
    "\n",
    "filename = 'train_adsCluster_' + str(train_val_split * 100) + '%'+'_'+ str(threshold) + '.p'\n",
    "pickle.dump(ads_kmeans,open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Add ad type to each user interaction record.'''\n",
    "train_validation_recordDFwType= pd.concat([train_recordsDF, \n",
    "                          train_recordsDF['adID'].map(train_adsType).rename('adsType')],axis =1).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Split train data into training and validation sets. Length ratio is given by train_val_split.'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_recordDFwType, validation_recordDFwType, = train_test_split(train_validation_recordDFwType,\n",
    "                                                                  test_size=train_val_split, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Calculate training set click probability for each pair of user type and article type in training set.'''\n",
    "train_recordDFwType['adsType'] = train_recordDFwType['adsType'].astype(int)\n",
    "train_Y_clickProb = train_recordDFwType[['click','userType','adsType']].groupby(['userType','adsType'])['click'].agg({'clickProb':'mean',\n",
    "                                                            'n_obs':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Drop article type 3 and several interaction records so average click probabilities for \n",
    "each user and article type were calculated with at least 50 interaction records. '''\n",
    "\n",
    "train_tmp = train_Y_clickProb['n_obs'].unstack().drop([3], axis =1).min(axis = 1)\n",
    "train_tmp_index = train_tmp[train_tmp >= threshold].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'filtered_train_clickprob_' + str(train_val_split * 100) + '%'+'_'+ str(threshold) + '.npy'\n",
    "np.save(filename, train_Y_clickProb['clickProb'].unstack().loc[train_tmp_index].drop([3],axis = 1).values) \n",
    "\n",
    "\n",
    "filename = 'filtered_train_usernumobserv_' + str(train_val_split * 100) + '%'+'_'+ str(threshold) + '.npy'\n",
    "np.save(filename, train_recordDFwType.groupby('userType').size().loc[train_tmp_index].values) \n",
    "\n",
    "\n",
    "train_X_user = train_recordDFwType.groupby(['userType']).mean().drop(['adID','click','adsType'],axis =1)\n",
    "filename = 'filtered_train_userFeat_' + str(train_val_split * 100) + '%'+'_'+ str(threshold) + '.npy'\n",
    "np.save(filename, train_X_user.loc[train_tmp_index].values)\n",
    "\n",
    "\n",
    "filename = 'filtered_train_featMap_' + str(train_val_split * 100) + '%'+'_'+ str(threshold) + '.npy'\n",
    "pickle.dump(dict(zip(list(train_X_user.index), train_X_user.values)), open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation set Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'filtered_train_featMap_' + str(train_val_split * 100) + '%'+'_'+ str(threshold) + '.npy'\n",
    "train_featMap = pickle.load(open(filename, 'rb'))\n",
    "validation_recordDFwType['adsType']  = validation_recordDFwType['adID'].map(train_adsType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Calculate training set click probability for each pair of user type and article type in validation set.'''\n",
    "validation_Y_clickProb = validation_recordDFwType[['click','userType','adsType']].groupby(['userType','adsType'])['click'].agg({'clickProb':'mean',\n",
    "                                                            'n_obs':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Drop article type 3 and several interaction records so average click probabilities for \n",
    "each user and article type were calculated with at least 50 interaction records. '''\n",
    "validation_tmp = validation_Y_clickProb['n_obs'].unstack().drop([3], axis =1).min(axis = 1)\n",
    "validation_tmp_index = validation_tmp[validation_tmp >= threshold].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'filtered_validation_clickprob_' + str(train_val_split * 100) + '%'+'_'+ str(threshold) + '.npy'\n",
    "np.save(filename, validation_Y_clickProb['clickProb'].unstack().loc[validation_tmp_index].drop([3],axis = 1).values) \n",
    "\n",
    "filename = 'filtered_validation_usernumobserv_' + str(train_val_split * 100) + '%'+'_'+ str(threshold) + '.npy'\n",
    "np.save(filename, validation_recordDFwType.groupby('userType').size().loc[validation_tmp_index].values) \n",
    "\n",
    "validation_X_user = np.vstack([train_featMap[x] for x in list(validation_tmp_index)])\n",
    "filename = 'filtered_validation_userFeat_' + str(train_val_split * 100) + '%'+'_'+ str(threshold) + '.npy'\n",
    "np.save(filename, validation_X_user) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Read in test data'''\n",
    "'''Col0 - Col4 are features, Col5 is adID, Col6 is click binary indicator.'''\n",
    "test_records = np.vstack([np.load('day' + str(day) + '_records.npy') for day in range(6,11)]) \n",
    "\n",
    "'''Read in ads dictionary.'''\n",
    "test_adsDict = dict()\n",
    "for day in range(6,11):\n",
    "    test_adsDict.update(pickle.load(open('day' + str(day) + '_adsDict.p','rb')))\n",
    "\n",
    "filename = 'train_userCluster.p'\n",
    "user_kmeans = pickle.load(open(filename,'rb'))\n",
    "\n",
    "filename = 'test_userTypePredictions.npy'\n",
    "test_usertype_predictions = np.load(filename)\n",
    "\n",
    "filename = 'train_adsCluster_' + str(train_val_split * 100) + '%'+'_'+ str(threshold) + '.p'\n",
    "ads_kmeans =  pickle.load(open(filename,'rb'))\n",
    "\n",
    "filename = 'filtered_train_featMap_' + str(train_val_split * 100) + '%'+'_'+ str(threshold) + '.npy'\n",
    "train_featMap = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_adsDF = pd.DataFrame(test_adsDict).T\n",
    "test_recordDFwType = pd.DataFrame(np.hstack([test_records,test_usertype_predictions.reshape(-1,1)])).rename(columns =                     \n",
    "                                                                                                            {5:'adID', 6: 'click',7: 'userType'})\n",
    "test_adsType = dict(zip(test_adsDF.index, ads_kmeans.predict(test_adsDF)))\n",
    "test_recordDFwType['adsType']  = test_recordDFwType['adID'].map(test_adsType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Calculate training set click probability for each pair of user type and article type in test set.'''\n",
    "test_Y_clickProb = test_recordDFwType[['click','userType','adsType']].groupby(['userType','adsType'])['click'].agg({'clickProb':'mean',\n",
    "                                                            'n_obs':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Drop article type 3 and several interaction records. '''\n",
    "test_tmp = test_Y_clickProb['n_obs'].unstack().drop([3], axis =1).min(axis = 1)\n",
    "test_tmp_index = test_tmp[test_tmp >= threshold].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'filtered_test_clickprob_' + str(train_val_split * 100) + '%'+'_'+ str(threshold) + '.npy'\n",
    "np.save(filename, test_Y_clickProb['clickProb'].unstack().loc[test_tmp_index].drop([3],axis = 1).values) \n",
    "\n",
    "filename = 'filtered_test_usernumobserv_' + str(train_val_split * 100) + '%'+'_'+ str(threshold) + '.npy'\n",
    "np.save(filename, test_recordDFwType.groupby('userType').size().loc[test_tmp_index].values) \n",
    "\n",
    "test_X_user = np.vstack([train_featMap[x] for x in list(test_tmp_index)])\n",
    "filename = 'filtered_test_userFeat_' + str(train_val_split * 100) + '%'+'_'+ str(threshold) + '.npy'\n",
    "np.save(filename, test_X_user) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
