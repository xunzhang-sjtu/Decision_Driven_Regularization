{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "import random\n",
    "random.seed(42)\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyepo\n",
    "# generate data\n",
    "grid = (5,5) # grid size\n",
    "num_data = 100 # number of training data\n",
    "num_test = 1000\n",
    "num_feat = 5 # size of feature\n",
    "deg = 4 # polynomial degree\n",
    "e = 0.5 # noise width\n",
    "feats, costs = pyepo.data.shortestpath.genData(num_data+num_test, num_feat, grid, deg, e, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build optModel\n",
    "from pyepo.model.grb import optGrbModel\n",
    "\n",
    "class shortestPathModel(optGrbModel):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.grid = (5,5)\n",
    "        self.arcs = self._getArcs()\n",
    "        super().__init__()\n",
    "\n",
    "    def _getArcs(self):\n",
    "        \"\"\"\n",
    "        A helper method to get list of arcs for grid network\n",
    "\n",
    "        Returns:\n",
    "            list: arcs\n",
    "        \"\"\"\n",
    "        arcs = []\n",
    "        for i in range(self.grid[0]):\n",
    "            # edges on rows\n",
    "            for j in range(self.grid[1] - 1):\n",
    "                v = i * self.grid[1] + j\n",
    "                arcs.append((v, v + 1))\n",
    "            # edges in columns\n",
    "            if i == self.grid[0] - 1:\n",
    "                continue\n",
    "            for j in range(self.grid[1]):\n",
    "                v = i * self.grid[1] + j\n",
    "                arcs.append((v, v + self.grid[1]))\n",
    "        return arcs\n",
    "\n",
    "    def _getModel(self):\n",
    "        \"\"\"\n",
    "        A method to build Gurobi model\n",
    "\n",
    "        Returns:\n",
    "            tuple: optimization model and variables\n",
    "        \"\"\"\n",
    "        import gurobipy as gp\n",
    "        from gurobipy import GRB\n",
    "        # ceate a model\n",
    "        m = gp.Model(\"shortest path\")\n",
    "        # varibles\n",
    "        x = m.addVars(self.arcs, name=\"x\")\n",
    "        # sense\n",
    "        m.modelSense = GRB.MINIMIZE\n",
    "        # flow conservation constraints\n",
    "        for i in range(self.grid[0]):\n",
    "            for j in range(self.grid[1]):\n",
    "                v = i * self.grid[1] + j\n",
    "                expr = 0\n",
    "                for e in self.arcs:\n",
    "                    # flow in\n",
    "                    if v == e[1]:\n",
    "                        expr += x[e]\n",
    "                    # flow out\n",
    "                    elif v == e[0]:\n",
    "                        expr -= x[e]\n",
    "                # source\n",
    "                if i == 0 and j == 0:\n",
    "                    m.addConstr(expr == -1)\n",
    "                # sink\n",
    "                elif i == self.grid[0] - 1 and j == self.grid[0] - 1:\n",
    "                    m.addConstr(expr == 1)\n",
    "                # transition\n",
    "                else:\n",
    "                    m.addConstr(expr == 0)\n",
    "        return m, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepara training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, c_train, c_test = train_test_split(feats, costs, test_size=num_test, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optmodel = shortestPathModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get optDataset\n",
    "dataset_train = pyepo.data.dataset.optDataset(optmodel, x_train, c_train)\n",
    "dataset_test = pyepo.data.dataset.optDataset(optmodel, x_test, c_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data loader\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 20\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=False)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# build linear model\n",
    "class LinearRegression(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(num_feat, (grid[0]-1)*grid[1]+(grid[1]-1)*grid[0])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial the predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# init model\n",
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyepo\n",
    "regret = pyepo.metric.regret(reg, optmodel, loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# train model\n",
    "def trainModel(reg, loss_func, method_name, num_epochs=20, lr=1e-2):\n",
    "    # set adam optimizer\n",
    "    optimizer = torch.optim.Adam(reg.parameters(), lr=lr)\n",
    "    # train mode\n",
    "    reg.train()\n",
    "    # init log\n",
    "    loss_log = []\n",
    "    loss_log_regret = [pyepo.metric.regret(reg, optmodel, loader_test)]\n",
    "    # print(\"epoch = \",0,\", regret = \",loss_log_regret[0])\n",
    "    # init elpased time\n",
    "    elapsed = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        # start timing\n",
    "        tick = time.time()\n",
    "        # load data\n",
    "        for i, data in enumerate(loader_train):\n",
    "            x, c, w, z = data\n",
    "            # cuda\n",
    "            if torch.cuda.is_available():\n",
    "                x, c, w, z = x.cuda(), c.cuda(), w.cuda(), z.cuda()\n",
    "            # forward pass\n",
    "            cp = reg(x)\n",
    "            if method_name == \"spo+\":\n",
    "                loss = loss_func(cp, c, w, z)\n",
    "            if method_name in [\"ptb\", \"pfy\", \"imle\", \"aimle\", \"nce\", \"cmap\"]:\n",
    "                loss = loss_func(cp, w)\n",
    "            if method_name in [\"dbb\", \"nid\"]:\n",
    "                loss = loss_func(cp, c, z)\n",
    "            if method_name in [\"pg\", \"ltr\"]:\n",
    "                loss = loss_func(cp, c)\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # record time\n",
    "            tock = time.time()\n",
    "            elapsed += tock - tick\n",
    "            # log\n",
    "            loss_log.append(loss.item())\n",
    "        regret = pyepo.metric.regret(reg, optmodel, loader_test)\n",
    "        print(\"epoch = \",epoch,\", regret = \",loss_log_regret[0])\n",
    "        loss_log_regret.append(regret)\n",
    "        print(\"Epoch {:2},  Loss: {:9.4f},  Regret: {:7.4f}%\".format(epoch+1, loss.item(), regret*100))\n",
    "    print(\"Total Elapsed Time: {:.2f} Sec.\".format(elapsed))\n",
    "    return loss_log, loss_log_regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPO+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# init model\n",
    "reg = LinearRegression()\n",
    "# cuda\n",
    "if torch.cuda.is_available():\n",
    "    reg = reg.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init SPO+ loss\n",
    "spop = pyepo.func.SPOPlus(optmodel, processes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_log, loss_log_regret = trainModel(reg, loss_func=spop, method_name=\"spo+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcs_arr = optmodel.arcs\n",
    "def obtain_path(arcs_arr,sol):\n",
    "    path_arr = []\n",
    "    for arc_index in range(len(arcs_arr)):\n",
    "        if sol[arc_index] > 0:\n",
    "            path_arr.append(arcs_arr[arc_index])\n",
    "    return path_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getArcs(grid):\n",
    "    arcs = []\n",
    "    for i in range(grid[0]):\n",
    "        # edges on rows\n",
    "        for j in range(grid[1] - 1):\n",
    "            v = i * grid[1] + j\n",
    "            arcs.append((v, v + 1))\n",
    "        # edges in columns\n",
    "        if i == grid[0] - 1:\n",
    "            continue\n",
    "        for j in range(grid[1]):\n",
    "            v = i * grid[1] + j\n",
    "            arcs.append((v, v + grid[1]))\n",
    "    return arcs\n",
    "\n",
    "def solve_Shortest_Path(arcs,cost):\n",
    "    \"\"\"\n",
    "    A method to build Gurobi model\n",
    "\n",
    "    Returns:\n",
    "        tuple: optimization model and variables\n",
    "    \"\"\"\n",
    "    import gurobipy as gp\n",
    "    from gurobipy import GRB\n",
    "    # ceate a model\n",
    "    m = gp.Model(\"shortest path\")\n",
    "    m.setParam('OutputFlag', 0)\n",
    "    # varibles\n",
    "    x = m.addVars(arcs, name=\"x\")\n",
    "    # sense\n",
    "    # m.modelSense = GRB.MINIMIZE\n",
    "    # flow conservation constraints\n",
    "    for i in range(grid[0]):\n",
    "        for j in range(grid[1]):\n",
    "            v = i * grid[1] + j\n",
    "            expr = 0\n",
    "            for e in arcs:\n",
    "                # flow in\n",
    "                if v == e[1]:\n",
    "                    expr += x[e]\n",
    "                # flow out\n",
    "                elif v == e[0]:\n",
    "                    expr -= x[e]\n",
    "            # source\n",
    "            if i == 0 and j == 0:\n",
    "                m.addConstr(expr == -1)\n",
    "            # sink\n",
    "            elif i == grid[0] - 1 and j == grid[0] - 1:\n",
    "                m.addConstr(expr == 1)\n",
    "            # transition\n",
    "            else:\n",
    "                m.addConstr(expr == 0)\n",
    "    m.setObjective( sum([cost[ind] * x[arcs_arr[ind]] for ind in range(len(arcs_arr))]) , GRB.MINIMIZE)\n",
    "    m.optimize()\n",
    "    sol = m.getAttr('x')\n",
    "    # print(\"sol = \",sol)\n",
    "    shortest_path = obtain_path(arcs_arr,sol)\n",
    "    \n",
    "    obj = m.getObjective().getValue()\n",
    "    # print(\"obj = \",obj,\"shortest_path = \",shortest_path)\n",
    "    return obj,sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyepo import EPO\n",
    "def regret(predmodel, optmodel, dataloader):\n",
    "    \"\"\"\n",
    "    A function to evaluate model performance with normalized true regret\n",
    "\n",
    "    Args:\n",
    "        predmodel (nn): a regression neural network for cost prediction\n",
    "        optmodel (optModel): an PyEPO optimization model\n",
    "        dataloader (DataLoader): Torch dataloader from optDataSet\n",
    "\n",
    "    Returns:\n",
    "        float: true regret loss\n",
    "    \"\"\"\n",
    "    # evaluate\n",
    "    predmodel.eval()\n",
    "    loss = 0\n",
    "    optsum = 0\n",
    "    # load data\n",
    "    iterations = 1\n",
    "    for data in dataloader:\n",
    "        x, c, w, z = data\n",
    "        # cuda\n",
    "        if next(predmodel.parameters()).is_cuda:\n",
    "            x, c, w, z = x.cuda(), c.cuda(), w.cuda(), z.cuda()\n",
    "            print(\"CUDA\")\n",
    "        # predict\n",
    "        with torch.no_grad(): # no grad\n",
    "            cp = predmodel(x).to(\"cpu\").detach().numpy()\n",
    "            # print(\"itereations = \",iterations,\",Hello World\")\n",
    "        # solve\n",
    "        for j in range(cp.shape[0]):\n",
    "            # accumulate loss\n",
    "            loss_tem,sol = calRegret(optmodel, cp[j], c[j].to(\"cpu\").detach().numpy(),\n",
    "                              z[j].item())\n",
    "            path_dict = obtain_path(arcs_arr,sol)\n",
    "            # print(\"j = \",j)\n",
    "            # print(\"x = \",x)\n",
    "            # print(\"predict cost = \",cp[j])\n",
    "            # print(\"sol = \",sol)\n",
    "            # print(\"ShortestPath = \",path_dict)\n",
    "            # print(\"loss = \",loss_tem)\n",
    "            # print()\n",
    "            loss = loss + loss_tem\n",
    "        optsum += abs(z).sum().item()\n",
    "        iterations = iterations + 1\n",
    "    # turn back train mode\n",
    "    predmodel.train()\n",
    "    # normalized\n",
    "    return loss / (optsum + 1e-7)\n",
    "\n",
    "\n",
    "def calRegret(optmodel, pred_cost, true_cost, true_obj):\n",
    "    \"\"\"\n",
    "    A function to calculate normalized true regret for a batch\n",
    "\n",
    "    Args:\n",
    "        optmodel (optModel): optimization model\n",
    "        pred_cost (torch.tensor): predicted costs\n",
    "        true_cost (torch.tensor): true costs\n",
    "        true_obj (torch.tensor): true optimal objective values\n",
    "\n",
    "    Returns:predmodel\n",
    "        float: true regret losses\n",
    "    \"\"\"\n",
    "    # opt sol for pred cost\n",
    "    optmodel.setObj(pred_cost)\n",
    "    sol, _ = optmodel.solve()\n",
    "    # obj with true cost\n",
    "    obj = np.dot(sol, true_cost)\n",
    "    # loss\n",
    "    if optmodel.modelSense == EPO.MINIMIZE:\n",
    "        loss = obj - true_obj\n",
    "    if optmodel.modelSense == EPO.MAXIMIZE:\n",
    "        loss = true_obj - obj\n",
    "    return loss,sol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set adam optimizer\n",
    "optimizer = torch.optim.Adam(reg.parameters(), lr=1e-2)\n",
    "# train mode\n",
    "reg.train()\n",
    "# init log\n",
    "loss_log = []\n",
    "regret(reg, optmodel, loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set adam optimizer\n",
    "optimizer = torch.optim.Adam(reg.parameters(), lr=1e-2)\n",
    "# train mode\n",
    "reg.train()\n",
    "# init log\n",
    "loss_log = []\n",
    "loss_log_regret = [pyepo.metric.regret(reg, optmodel, loader_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regret(reg, optmodel, loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_oracle = []\n",
    "cost_pred_prev = []\n",
    "for i, data in enumerate(loader_test):\n",
    "    x, c, w, z = data\n",
    "    cost_predict = reg(x)\n",
    "    \n",
    "    sample_length = np.shape(c)[0]\n",
    "    # print(\"sample_length = \",sample_length)\n",
    "    w_all = w.numpy()\n",
    "    cost_true = c.numpy()\n",
    "    x_all = x.numpy()\n",
    "    print(\"z = \",z.numpy()[:,0])\n",
    "    cost_true_arr = np.zeros(sample_length)\n",
    "    for sample_index in range(sample_length):\n",
    "        # print(\"======= sample index = \",sample_index,\"=============\")\n",
    "        # print(\"w Shortest_Path = \",obtain_path(arcs_arr,w_all[sample_index]))\n",
    "        obj_true,sol_true = solve_Shortest_Path(arcs_arr,cost_true[sample_index])\n",
    "        cost_true_arr[sample_index] = obj_true\n",
    "    print(\"obj_true = \",cost_true_arr)\n",
    "        # obj_pred,sol_pred = solve_Shortest_Path(arcs_arr,cost_predict[sample_index])\n",
    "        # cost_oracle.append(obj_true)\n",
    "        # cost_pred_prev.append(obj_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cost_pred_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# init elpased time\n",
    "elapsed = 0\n",
    "for epoch in range(2):\n",
    "    # start timing\n",
    "    tick = time.time()\n",
    "    # load data\n",
    "    for i, data in enumerate(loader_train):\n",
    "        x, c, w, z = data\n",
    "        # sample_length = np.shape(c)[0]\n",
    "        # w_all = w.numpy()\n",
    "        # cost_all = c.numpy()\n",
    "        # for sample_index in range(1):\n",
    "        #     cost = cost_all[sample_index]\n",
    "        #     print(\"======= sample index = \",sample_index,\"=============\")\n",
    "        #     print(\"w Shortest_Path = \",obtain_path(arcs_arr,w_all[sample_index]))\n",
    "        #     solve_Shortest_Path(arcs_arr,cost)\n",
    "            \n",
    "        # cuda\n",
    "        if torch.cuda.is_available():\n",
    "            x, c, w, z = x.cuda(), c.cuda(), w.cuda(), z.cuda()\n",
    "        # print(\"x = \",x)\n",
    "        # forward pass\n",
    "        cp = reg(x)\n",
    "\n",
    "        loss = spop(cp, c, w, z)\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # record time\n",
    "        tock = time.time()\n",
    "        elapsed += tock - tick\n",
    "        # log\n",
    "        loss_log.append(loss.item())\n",
    "        regret(reg, optmodel, loader_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
