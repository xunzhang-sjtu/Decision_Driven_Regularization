{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4865f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "from Performance import performance_evaluation\n",
    "perfs = performance_evaluation()\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "from matplotlib import gridspec\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79716a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Simulator(DataPath,lower, upper, p, d, coef_seed,iteration_all,num_test, num_train, alpha,mis,data_generation_process,x_dist, e_dist, x_low, x_up, x_mean, x_var, bump):\n",
    "    from Data import data_generation\n",
    "    data_gen = data_generation()\n",
    "    # W_star = data_gen.generate_truth(DataPath,lower, upper, p, d, coef_seed,data_generation_process) \n",
    "    # print(\"W_star = \",W_star[0,:])\n",
    "    np.random.seed(coef_seed)\n",
    "    # random.seed(coef_seed)\n",
    "    # seed_arr = random.sample(range(1, 10000 + 1), len(iteration_all))\n",
    "\n",
    "    seed_arr = [coef_seed for k in range(100)]\n",
    "    x_test_all = {}; c_test_all = {}; x_train_all = {}; c_train_all = {}; W_star_all = {}; noise_train_all = {}; noise_test_all = {}\n",
    "    for iter in iteration_all:\n",
    "        DataPath_iter = DataPath +\"iter=\"+str(iter)+\"/\"\n",
    "        pathlib.Path(DataPath_iter).mkdir(parents=True, exist_ok=True)\n",
    "        W_star = data_gen.generate_truth(DataPath_iter,lower, upper, p, d, iter,data_generation_process) \n",
    "        # print(\"W_star=\",W_star[0,:])\n",
    "        x_test_all[iter], c_test_all[iter], x_train_all[iter], c_train_all[iter], noise_train_all[iter],noise_test_all[iter],W_star_all[iter] \\\n",
    "        = data_gen.generate_samples(iter,DataPath_iter,p, d, num_test, num_train, alpha, W_star, mis, num_test, data_generation_process, x_dist, e_dist, x_low, x_up, x_mean, x_var, bump) \n",
    "    return x_test_all, c_test_all, x_train_all, c_train_all, noise_train_all,noise_test_all,W_star_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26433e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Implement_Oracle(arcs, grid,mis,bump,W_star_all,x_test_all,noise_test_all,iteration_all,num_feat,data_generation_process):\n",
    "    cost_Oracle_Post = {}; cost_Oracle_Ante = {}\n",
    "    for iter in iteration_all:\n",
    "        if data_generation_process == \"SPO_Data_Generation\":\n",
    "            cost_oracle_ori = (W_star_all[iter] @ x_test_all[iter].T)/np.sqrt(num_feat) + 3\n",
    "            cost_oracle_pred = (cost_oracle_ori ** mis + 1).T\n",
    "            # cost_Oracle_Post[iter] = perfs.compute_SPO_out_of_sample_Cost_Ex_Post(arcs, grid,cost_oracle_pred,cost_oracle_pred,noise_test_all[iter])\n",
    "            cost_Oracle_Ante[iter] = perfs.compute_SPO_out_of_sample_Cost_Ex_Ante(arcs, grid,cost_oracle_pred,cost_oracle_pred)\n",
    "\n",
    "        if data_generation_process == \"DDR_Data_Generation\":\n",
    "            cost_oracle_ori = (W_star_all[iter] @ x_test_all[iter].T) + bump\n",
    "            cost_oracle_pred = (cost_oracle_ori ** mis).T\n",
    "            cost_Oracle_Ante[iter] = perfs.compute_SPO_out_of_sample_Cost_Ex_Ante(arcs, grid,cost_oracle_pred,cost_oracle_pred)\n",
    "            \n",
    "        if iter % 20 == 0 and iter > 0:\n",
    "            print(\"Oracle: iter=\",iter,\",cost_Oracle_Ante=\",np.nanmean(cost_Oracle_Ante[iter]))\n",
    "            # print(\"Oracle: iter=\",iter,\",cost_Oracle_Post=\",np.nanmean(cost_Oracle_Post[iter]),\",cost_Oracle_Ante=\",np.nanmean(cost_Oracle_Ante[iter]))\n",
    "    return cost_Oracle_Post,cost_Oracle_Ante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa761e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Implement_OLS(arcs, grid,mis,bump,W_star_all,x_test_all,noise_test_all,x_train_all,c_train_all,iteration_all,num_feat,data_generation_process):\n",
    "    from OLS import ols_method\n",
    "    ols_method_obj = ols_method()\n",
    "    W_ols_all = {}; w0_ols_all = {}; t_ols_all = {}; obj_ols_all = {}\n",
    "    cost_OLS_Post = {}; cost_OLS_Ante = {}\n",
    "    for iter in iteration_all:\n",
    "        # compute OLS performance\n",
    "        W_ols_all[iter], w0_ols_all[iter], t_ols_all[iter], obj_ols_all[iter] = ols_method_obj.ols_solver(\"\",x_train_all[iter], c_train_all[iter])\n",
    "        cost_dem = (W_ols_all[iter] @ x_test_all[iter].T).T + w0_ols_all[iter]\n",
    "\n",
    "        if data_generation_process == \"SPO_Data_Generation\":\n",
    "            cost_oracle_ori = (W_star_all[iter] @ x_test_all[iter].T)/np.sqrt(num_feat) + 3\n",
    "            cost_oracle_pred = (cost_oracle_ori ** mis + 1).T\n",
    "            # cost_OLS_Post[iter] = perfs.compute_SPO_out_of_sample_Cost_Ex_Post(arcs, grid,cost_dem,cost_oracle_pred,noise_test_all[iter])\n",
    "            cost_OLS_Ante[iter] = perfs.compute_SPO_out_of_sample_Cost_Ex_Ante(arcs, grid,cost_dem,cost_oracle_pred)\n",
    "\n",
    "        if data_generation_process == \"DDR_Data_Generation\":\n",
    "            cost_oracle_ori = (W_star_all[iter] @ x_test_all[iter].T) + bump\n",
    "            cost_oracle_pred = (cost_oracle_ori ** mis).T\n",
    "            cost_OLS_Ante[iter] = perfs.compute_SPO_out_of_sample_Cost_Ex_Ante(arcs, grid,cost_dem,cost_oracle_pred)\n",
    "\n",
    "        if iter % 20 == 0 and iter > 0:\n",
    "            print(\"OLS: iter=\",iter,\",cost_OLS_Ante=\",np.nanmean(cost_OLS_Ante[iter]))\n",
    "            # print(\"OLS: iter=\",iter,\",cost_OLS_Post =\",np.nanmean(cost_OLS_Post[iter]),\",cost_OLS_Ante=\",np.nanmean(cost_OLS_Ante[iter]))\n",
    "    return cost_OLS_Post,cost_OLS_Ante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87a351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Implement_DDR(mu_all,lamb_all,arcs, grid,mis,bump,W_star_all,x_test_all,noise_test_all,x_train_all,c_train_all,iteration_all,num_feat,data_generation_process):\n",
    "    from DDR import DDR_method\n",
    "    ddr_object = DDR_method()\n",
    "    num_nodes = grid[0] * grid[0]\n",
    "\n",
    "    w0_ddr_dict = {}; W_ddr_dict = {}\n",
    "    cost_DDR_Post = {}; cost_DDR_Ante = {}\n",
    "    for iter in iteration_all:\n",
    "        for mu in mu_all:\n",
    "            for lamb in lamb_all:\n",
    "                w0_ddr_dict[iter,mu,lamb],W_ddr_dict[iter,mu,lamb],alpha_rst,obj_ddr = ddr_object.solve_DDR(arcs,lamb,mu,num_nodes,x_train_all[iter],c_train_all[iter])\n",
    "                cost_pred = (W_ddr_dict[iter,mu,lamb] @ x_test_all[iter].T).T + w0_ddr_dict[iter,mu,lamb]\n",
    "                if data_generation_process == \"SPO_Data_Generation\":\n",
    "                    cost_oracle_ori = (W_star_all[iter] @ x_test_all[iter].T)/np.sqrt(num_feat) + 3\n",
    "                    cost_oracle_pred = (cost_oracle_ori ** mis + 1).T\n",
    "                    # cost_DDR_Post[iter,mu,lamb] = perfs.compute_SPO_out_of_sample_Cost_Ex_Post(arcs, grid,cost_pred,cost_oracle_pred,noise_test_all[iter])\n",
    "                    cost_DDR_Ante[iter,mu,lamb] = perfs.compute_SPO_out_of_sample_Cost_Ex_Ante(arcs, grid,cost_pred,cost_oracle_pred)\n",
    "\n",
    "                if data_generation_process == \"DDR_Data_Generation\":\n",
    "                    cost_oracle_ori = (W_star_all[iter] @ x_test_all[iter].T) + bump\n",
    "                    cost_oracle_pred = (cost_oracle_ori ** mis).T\n",
    "                    cost_DDR_Ante[iter,mu,lamb] = perfs.compute_SPO_out_of_sample_Cost_Ex_Ante(arcs, grid,cost_pred,cost_oracle_pred)\n",
    "\n",
    "        if iter % 20 == 0 and iter > 0:\n",
    "            print(\"DDR: iter=\",iter,\",mu=\",mu,\",lamb=\",lamb,\",cost_DDR_Ante =\",np.nanmean(cost_DDR_Ante[iter,mu,lamb]))\n",
    "            # print(\"DDR: iter=\",iter,\",mu=\",mu,\",lamb=\",lamb,\",cost_DDR_Post =\",np.nanmean(cost_DDR_Post[iter,mu,lamb]),\n",
    "            #       \",cost_DDR_Ante =\",np.nanmean(cost_DDR_Ante[iter,mu,lamb]))\n",
    "    return cost_DDR_Post,cost_DDR_Ante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5214c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Implement_EPO(DataPath,iteration_all,batch_size,num_epochs,method_names,W_star_all,bump,x_train_all,c_train_all,x_test_all,noise_test_all,\\\n",
    "                  arcs,grid,epo_runner,perfs,num_feat,mis,data_generation_process):\n",
    "    W_EPO_all = {}; w0_EPO_all = {}\n",
    "    cost_EPO_Post = {}; cost_EPO_Ante = {}\n",
    "    for iter in iteration_all:\n",
    "        DataPath_seed = DataPath +\"iter=\"+str(iter)+\"/\"\n",
    "        pathlib.Path(DataPath_seed).mkdir(parents=True, exist_ok=True)\n",
    "        W_EPO_all[iter],w0_EPO_all[iter] = epo_runner.run(method_names,DataPath_seed,batch_size,num_feat,grid,num_epochs,\\\n",
    "                                        x_train_all[iter],c_train_all[iter],arcs)\n",
    "        \n",
    "        cost_pred = (W_EPO_all[iter] @ x_test_all[iter].T).T + w0_EPO_all[iter]\n",
    "        if data_generation_process == \"SPO_Data_Generation\":\n",
    "            cost_oracle_ori = (W_star_all[iter] @ x_test_all[iter].T)/np.sqrt(num_feat) + 3\n",
    "            non_negative_cols = (cost_oracle_ori > 0).all(axis=0)\n",
    "            cost_oracle_ori = cost_oracle_ori[:,non_negative_cols]\n",
    "            cost_oracle_pred = (cost_oracle_ori ** mis + 1).T\n",
    "            \n",
    "            cost_pred = cost_pred[non_negative_cols,:]\n",
    "            # cost_EPO_Post[iter] = perfs.compute_SPO_out_of_sample_Cost_Ex_Post(arcs, grid,cost_pred,cost_oracle_pred,noise_test_all[iter])\n",
    "            cost_EPO_Ante[iter] = perfs.compute_SPO_out_of_sample_Cost_Ex_Ante(arcs, grid,cost_pred,cost_oracle_pred)\n",
    "\n",
    "        if data_generation_process == \"DDR_Data_Generation\":\n",
    "            cost_oracle_ori = (W_star_all[iter] @ x_test_all[iter].T) + bump\n",
    "            cost_oracle_pred = (cost_oracle_ori ** mis).T\n",
    "            cost_EPO_Ante[iter] = perfs.compute_SPO_out_of_sample_Cost_Ex_Ante(arcs, grid,cost_pred,cost_oracle_pred)\n",
    "\n",
    "        if iter % 20 == 0 and iter > 0:\n",
    "            print(method_names,\": iter=\",iter,\",cost_EPO_Ante=\",np.nanmean(cost_EPO_Ante[iter]))\n",
    "            # print(method_names,\": iter=\",iter,\",cost_EPO_Post =\",np.nanmean(cost_EPO_Post[iter]),\",cost_EPO_Ante=\",np.nanmean(cost_EPO_Ante[iter]))\n",
    "    return cost_EPO_Post,cost_EPO_Ante"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a9c70a",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a074b450",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 100 # number of training data\n",
    "num_feat = 5 # size of feature\n",
    "num_test = 1000\n",
    "deg = 1.0 # polynomial degree\n",
    "e = 0.5 # scale of normal std or the range of uniform. For the error term\n",
    "\n",
    "lower = 0 # coef lower bound\n",
    "upper = 1 # coef upper bound\n",
    "p = num_feat # num of features\n",
    "# d = (grid[0] - 1) * (grid[0] - 1) * 2 + 2 * (grid[0] - 1) # num of arcs\n",
    "# num_nodes = grid[0]*grid[0]\n",
    "alpha = e # scale of normal std or the range of uniform. For the error term\n",
    "mis = deg # model misspecification\n",
    "coef_seed = 1\n",
    "\n",
    "x_dist = 'uniform'\n",
    "e_dist = 'normal'\n",
    "x_low = -2\n",
    "x_up = 2\n",
    "x_mean = 2\n",
    "x_var = 2\n",
    "bump = 100\n",
    "\n",
    "# data_generation_process = \"SPO_Data_Generation\"\n",
    "data_generation_process = \"DDR_Data_Generation\"\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "project_directory = os.path.dirname(os.path.dirname(os.path.dirname(parent_directory)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db0d6ee",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7c12f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_all = [(2,2),(3,3),(4,4),(5,5)]\n",
    "middle_path = '/Data_JOC_R1/Shortest_Path/Baseline_Final/'\n",
    "x_test_grid = {}; c_test_grid = {}; x_train_grid = {}; c_train_grid = {}; noise_train_grid = {}; noise_test_grid = {}; W_star_grid = {}\n",
    "iteration_all = np.arange(0,100)\n",
    "for grid in grid_all:\n",
    "\n",
    "    from Network import network_design\n",
    "    Network = network_design()\n",
    "    arcs,arc_index_mapping = Network._getArcs(grid)\n",
    "    L_N = grid[0]\n",
    "    d = (L_N - 1) * (L_N - 1) * 2 + 2 * (L_N - 1) # num of arcs\n",
    "    num_nodes = L_N*L_N\n",
    "    DataPath_parent = project_directory + middle_path +str(L_N)+'by'+str(L_N)+'_grid_' + data_generation_process + \"/\"\n",
    "    DataPath = DataPath_parent + \"data_size=\"+str(num_train)+\"_deg=\"+str(deg)+\"_e=\"+str(e)+\"_d=\"+str(d)+\"_x_dist=\"+x_dist+\"_num_test=\"+str(num_test)+\"_same_W/\"\n",
    "    pathlib.Path(DataPath).mkdir(parents=True, exist_ok=True)\n",
    "    print(\"DataPath:\", DataPath)\n",
    "\n",
    "    x_test_grid[L_N], c_test_grid[L_N], x_train_grid[L_N], c_train_grid[L_N],noise_train_grid[L_N],noise_test_grid[L_N],W_star_grid[L_N] \\\n",
    "    = Data_Simulator(DataPath,lower, upper, p, d, coef_seed,iteration_all,num_test, num_train, alpha,mis,data_generation_process,x_dist, e_dist, x_low, x_up, x_mean, x_var, bump)\n",
    "\n",
    "    with open(DataPath+'x_test_grid.pkl', \"wb\") as tf:\n",
    "        pickle.dump(x_test_grid[L_N],tf)\n",
    "    with open(DataPath+'c_test_grid.pkl', \"wb\") as tf:\n",
    "        pickle.dump(c_test_grid[L_N],tf)\n",
    "    with open(DataPath+'x_train_grid.pkl', \"wb\") as tf:\n",
    "        pickle.dump(x_train_grid[L_N],tf)\n",
    "    with open(DataPath+'c_train_grid.pkl', \"wb\") as tf:\n",
    "        pickle.dump(c_train_grid[L_N],tf)\n",
    "    with open(DataPath+'noise_train_grid.pkl', \"wb\") as tf:\n",
    "        pickle.dump(noise_train_grid[L_N],tf)\n",
    "    with open(DataPath+'noise_test_grid.pkl', \"wb\") as tf:\n",
    "        pickle.dump(noise_test_grid[L_N],tf)\n",
    "    with open(DataPath+'W_star_grid.pkl', \"wb\") as tf:\n",
    "        pickle.dump(W_star_grid[L_N],tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d760b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Result_dir = project_directory + middle_path + \"Result_\" + data_generation_process + \"/\"\n",
    "pathlib.Path(Result_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cost_Oracle_Post_all = {}; cost_Oracle_Ante_all = {}\n",
    "cost_OLS_Post_all = {}; cost_OLS_Ante_all = {}\n",
    "cost_DDR_Post_all = {}; cost_DDR_Ante_all = {}\n",
    "\n",
    "for grid in grid_all:\n",
    "    from Network import network_design\n",
    "    Network = network_design()\n",
    "    arcs,arc_index_mapping = Network._getArcs(grid)\n",
    "\n",
    "    print(\"grid = \",grid)\n",
    "    L_N = grid[0]\n",
    "    d = (L_N - 1) * (L_N - 1) * 2 + 2 * (L_N - 1) # num of arcs\n",
    "    num_nodes = L_N*L_N\n",
    "    DataPath_parent = project_directory + middle_path +str(L_N)+'by'+str(L_N)+'_grid_' + data_generation_process + \"/\"\n",
    "    DataPath = DataPath_parent + \"data_size=\"+str(num_train)+\"_deg=\"+str(deg)+\"_e=\"+str(e)+\"_d=\"+str(d)+\"_x_dist=\"+x_dist+\"_num_test=\"+str(num_test)+\"_diff_W/\"\n",
    "    print(\"DataPath:\", DataPath)\n",
    "\n",
    "    cost_Oracle_Post_all[L_N],cost_Oracle_Ante_all[L_N] = Implement_Oracle(arcs, grid,mis,bump,\\\n",
    "                                                        W_star_grid[L_N],x_test_grid[L_N],noise_test_grid[L_N],\\\n",
    "                                                        iteration_all,num_feat,data_generation_process)\n",
    "\n",
    "    cost_OLS_Post_all[L_N],cost_OLS_Ante_all[L_N] = Implement_OLS(arcs, grid,mis,bump,\\\n",
    "                                                W_star_grid[L_N],x_test_grid[L_N],noise_test_grid[L_N],x_train_grid[L_N],c_train_grid[L_N],\\\n",
    "                                                iteration_all,num_feat,data_generation_process)\n",
    "\n",
    "\n",
    "    mu_all = np.round(np.arange(0.1,1.0,0.1),4)\n",
    "    lamb_all = np.round(np.arange(0.0,1.0,0.1),4)\n",
    "    cost_DDR_Post_all[L_N],cost_DDR_Ante_all[L_N] = Implement_DDR(mu_all,lamb_all,arcs, grid,mis,bump,\\\n",
    "                                                                  W_star_grid[L_N],x_test_grid[L_N],noise_test_grid[L_N],x_train_grid[L_N],c_train_grid[L_N],\\\n",
    "                                                                    iteration_all,num_feat,data_generation_process)\n",
    "\n",
    "with open(Result_dir+'cost_Oracle_Post_all.pkl', \"wb\") as tf:\n",
    "    pickle.dump(cost_Oracle_Post_all,tf)\n",
    "with open(Result_dir+'cost_Oracle_Ante_all.pkl', \"wb\") as tf:\n",
    "    pickle.dump(cost_Oracle_Ante_all,tf)\n",
    "\n",
    "with open(Result_dir+'cost_OLS_Post_all.pkl', \"wb\") as tf:\n",
    "    pickle.dump(cost_OLS_Post_all,tf)\n",
    "with open(Result_dir+'cost_OLS_Ante_all.pkl', \"wb\") as tf:\n",
    "    pickle.dump(cost_OLS_Ante_all,tf)\n",
    "\n",
    "with open(Result_dir+'cost_DDR_Post_all.pkl', \"wb\") as tf:\n",
    "    pickle.dump(cost_DDR_Post_all,tf)\n",
    "with open(Result_dir+'cost_DDR_Ante_all.pkl', \"wb\") as tf:\n",
    "    pickle.dump(cost_DDR_Ante_all,tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f9577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_SPO_Post_all = {}; cost_SPO_Ante_all = {}\n",
    "for grid in grid_all:\n",
    "    from Network import network_design\n",
    "    Network = network_design()\n",
    "    arcs,arc_index_mapping = Network._getArcs(grid)\n",
    "\n",
    "    L_N = grid[0]\n",
    "    d = (L_N - 1) * (L_N - 1) * 2 + 2 * (L_N - 1) # num of arcs\n",
    "    num_nodes = L_N*L_N\n",
    "    DataPath_parent = project_directory + middle_path +str(L_N)+'by'+str(L_N)+'_grid_' + data_generation_process + \"/\"\n",
    "    DataPath = DataPath_parent + \"data_size=\"+str(num_train)+\"_deg=\"+str(deg)+\"_e=\"+str(e)+\"_d=\"+str(d)+\"_x_dist=\"+x_dist+\"_num_test=\"+str(num_test)+\"_diff_W/\"\n",
    "    print(\"DataPath:\", DataPath)\n",
    "\n",
    "    batch_size = 20\n",
    "    num_epochs = 1000\n",
    "    from PYEPO import PyEPO_Method\n",
    "    epo_runner = PyEPO_Method()\n",
    "    method_names = [\"spo+\"]\n",
    "    cost_SPO_Post_all[L_N],cost_SPO_Ante_all[L_N] = Implement_EPO(DataPath,iteration_all,batch_size,num_epochs,method_names,\\\n",
    "                                                W_star_grid[L_N],bump,x_train_grid[L_N],c_train_grid[L_N],x_test_grid[L_N],noise_test_grid[L_N],\\\n",
    "                                                arcs,grid,epo_runner,perfs,num_feat,mis,data_generation_process)\n",
    "with open(Result_dir+'cost_SPO_Post_all.pkl', \"wb\") as tf:\n",
    "    pickle.dump(cost_SPO_Post_all,tf)\n",
    "with open(Result_dir+'cost_SPO_Ante_all.pkl', \"wb\") as tf:\n",
    "    pickle.dump(cost_SPO_Ante_all,tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b640b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(Result_dir+'cost_Oracle_Ante_all.pkl', \"rb\") as tf:\n",
    "#     cost_Oracle_Ante_all = pickle.load(tf)\n",
    "\n",
    "# with open(Result_dir+'cost_OLS_Ante_all.pkl', \"rb\") as tf:\n",
    "#     cost_OLS_Ante_all = pickle.load(tf)\n",
    "\n",
    "# with open(Result_dir+'cost_DDR_Ante_all.pkl', \"rb\") as tf:\n",
    "#     cost_DDR_Ante_all = pickle.dump(tf)\n",
    "\n",
    "# with open(Result_dir+'cost_SPO_Ante_all.pkl', \"rb\") as tf:\n",
    "#     cost_SPO_Ante_all = pickle.dump(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522b42a1",
   "metadata": {},
   "source": [
    "## Compare DDR vs OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4764333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_compare2plus(c_item, c_base, c_oracle):\n",
    "    N = len(c_item)\n",
    "    c_diff = c_base - c_item\n",
    "    lbel = np.zeros((N,1))\n",
    "    \n",
    "    equals = np.sum(c_diff == 0)\n",
    "    wins = np.sum(c_diff > 0) # indicate num of c_item is lower than c_base\n",
    "    lose = np.sum(c_diff < 0)\n",
    "    \n",
    "    lbel[c_diff < 0] = 1\n",
    "    lbel[c_diff > 0] = -1\n",
    "    \n",
    "#     print(N, equals, wins, lose)\n",
    "    if N == equals:\n",
    "        win_ratio = 0.5\n",
    "    else:\n",
    "        win_ratio = wins/(N - equals)\n",
    "    cost_reduction = (np.nanmean(c_diff))/np.abs(np.nanmean(c_base))\n",
    "    if np.nanmean(c_base) - np.nanmean(c_oracle) <= 1e-6:\n",
    "        regret_reduction = 0.0\n",
    "    else:\n",
    "        regret_reduction = (np.nanmean(c_diff))/np.abs(np.nanmean(c_base) - np.nanmean(c_oracle))\n",
    "    return win_ratio, cost_reduction, regret_reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f06a64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_H2H_Regret(iteration_all,c_item_all, c_base_all, c_oracle_all):\n",
    "    h2h = np.zeros(len(iteration_all)); cost_rd = np.zeros(len(iteration_all)); regret_rd = np.zeros(len(iteration_all))\n",
    "    for iter_index in range(len(iteration_all)):\n",
    "        iter = iteration_all[iter_index]\n",
    "        h2h[iter_index],cost_rd[iter_index],regret_rd[iter_index] = cross_compare2plus(c_item_all[iter], c_base_all[iter], c_oracle_all[iter])\n",
    "    return h2h,cost_rd,regret_rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afb5a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "h2h_ddr_vs_ols_post = {}; cost_rd_ddr_vs_ols_post = {}; regret_rd_ddr_vs_ols_post = {}\n",
    "h2h_ddr_vs_ols_ante = {}; cost_rd_ddr_vs_ols_ante = {}; regret_rd_ddr_vs_ols_ante = {}\n",
    "\n",
    "h2h_ddr_vs_spo_ante = {}; cost_rd_ddr_vs_spo_ante = {}; regret_rd_ddr_vs_spo_ante = {}\n",
    "\n",
    "for grid in grid_all:\n",
    "    L_N = grid[0]\n",
    "    for mu in mu_all:\n",
    "        for lamb in lamb_all:\n",
    "            cost_ddr_post_tem = {}; cost_ddr_ante_tem = {}\n",
    "            for iter_index in range(len(iteration_all)):\n",
    "                iter = iteration_all[iter_index]\n",
    "                # cost_ddr_post_tem[iter] = cost_DDR_Post_all[L_N][iter,mu,lamb]\n",
    "                cost_ddr_ante_tem[iter] = cost_DDR_Ante_all[L_N][iter,mu,lamb]\n",
    "\n",
    "            # h2h_ddr_vs_ols_post[L_N,mu,lamb],cost_rd_ddr_vs_ols_post[L_N,mu,lamb], regret_rd_ddr_vs_ols_post[L_N,mu,lamb] \\\n",
    "            #     = calculate_H2H_Regret(iteration_all,cost_ddr_post_tem, cost_OLS_Post_all[L_N], cost_Oracle_Post_all[L_N])\n",
    "            \n",
    "            h2h_ddr_vs_ols_ante[L_N,mu,lamb],cost_rd_ddr_vs_ols_ante[L_N,mu,lamb], regret_rd_ddr_vs_ols_ante[L_N,mu,lamb] \\\n",
    "                = calculate_H2H_Regret(iteration_all,cost_ddr_ante_tem, cost_OLS_Ante_all[L_N], cost_Oracle_Ante_all[L_N])\n",
    "\n",
    "            h2h_ddr_vs_spo_ante[L_N,mu,lamb],cost_rd_ddr_vs_spo_ante[L_N,mu,lamb], regret_rd_ddr_vs_spo_ante[L_N,mu,lamb] \\\n",
    "                = calculate_H2H_Regret(iteration_all,cost_ddr_ante_tem, cost_SPO_Ante_all[L_N], cost_Oracle_Ante_all[L_N])\n",
    "    print(\"Grid = \",grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc37252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4770783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure_plot_upright(all_x, all_y, figure_name, size = (5, 5), move = [-0.07, 0.07, 0.35, 0.55], \n",
    "                        ysame = 0, yrange = [6,6], sublabel = '', ypio = 1):\n",
    "    \n",
    "    data = np.asarray([all_x,all_y])\n",
    "\n",
    "    xmin, ymin = data.min(axis = 1)\n",
    "    xmax, ymax = data.max(axis = 1)\n",
    "\n",
    "    xmax, xmin = tuple(np.array([xmax, xmin]) + 0.1*(xmax - xmin)*np.array([1, -1]))\n",
    "    ymax, ymin = tuple(np.array([ymax, ymin]) + 0.1*(ymax - ymin)*np.array([1, -1]))\n",
    "\n",
    "    ####### Obtain KDE  \n",
    "\n",
    "    #KDE for top marginal\n",
    "    kde_X = gaussian_kde(data[0])\n",
    "    #KDE for right marginal\n",
    "    kde_Y = gaussian_kde(data[1])\n",
    "\n",
    "    x = np.linspace(0, 100, 100)\n",
    "    y = np.linspace(ymin, ymax, 100)\n",
    "\n",
    "    dx = kde_X(x) # X-marginal density\n",
    "    dy = kde_Y(y) # Y-marginal density\n",
    "\n",
    "    #Define grid for subplots\n",
    "    gs = gridspec.GridSpec(2, 2, width_ratios=[3, 1], height_ratios = [1, 3])\n",
    "\n",
    "    ####### Create scatter plot\n",
    "    fig = plt.figure(figsize = size)\n",
    "    ax = plt.subplot(gs[1, 0])\n",
    "    cax = ax.scatter(data[0], data[1], s = 15, color='#003D7C', marker = \"o\", edgecolors = \"#EF7C00\")\n",
    "    plt.xlabel('Head-to-head (%)')\n",
    "\n",
    "    plt.ylabel('Regret reduction (%)') #pio\n",
    "    \n",
    "    \n",
    "    if ysame == 0:\n",
    "        plt.vlines(50, ymin, ymax, linestyle=\"dashed\", alpha = 0.8,color = 'k')\n",
    "    else:\n",
    "        plt.vlines(50, yrange[0], yrange[1], linestyle=\"dashed\", alpha = 0.8,color = 'k')\n",
    "    \n",
    "    if ypio == 0:\n",
    "        plt.hlines(0, 0, 100, linestyle=\"dashed\", alpha = 0.8,color = 'k')\n",
    "        ax.annotate(sublabel, xy = (0.55,0.9), xycoords = 'axes fraction', bbox=dict(boxstyle=\"round\", fc=\"w\"), size = 10)\n",
    "    elif ypio == 1:\n",
    "        plt.hlines(0, 0, 100, linestyle=\"dashed\", alpha = 0.8,color = 'k')\n",
    "        ax.annotate(sublabel, xy = (0.05,0.9), xycoords = 'axes fraction', bbox=dict(boxstyle=\"round\", fc=\"w\"), size = 10)\n",
    "    else:\n",
    "        plt.hlines(100, 0, 100, linestyle=\"dashed\", alpha = 0.8,color = 'k') #pio\n",
    "        ax.annotate(sublabel, xy = (0.55,0.9), xycoords = 'axes fraction', bbox=dict(boxstyle=\"round\", fc=\"w\"), size = 10)\n",
    "\n",
    "    ####### Create Y-marginal (right)\n",
    "    axr = plt.subplot(gs[1, 1], xticks = [], yticks = [], frameon = False)\n",
    "    axr.plot(dy, y, color = 'black')\n",
    "\n",
    "    if ypio == 0:\n",
    "        axr.fill_betweenx(y, 0, dy, where = y <= 0.01, alpha = 1, color='#EF7C00')\n",
    "        axr.fill_betweenx(y, 0, dy, where = y >= 0, alpha = 1, color='#003D7C')\n",
    "\n",
    "        leftarea = np.round( sum(n <= 0 for n in all_y)/len(all_y),2 )\n",
    "        rightarea = np.round( sum(n > 0 for n in all_y)/len(all_y),2 )\n",
    "\n",
    "        axr.annotate(leftarea, xy=(0.15, abs(ymin)/(ymax - ymin) + move[0]), xycoords='axes fraction', bbox=dict(boxstyle=\"round\", fc=\"w\"), size = 12)\n",
    "        axr.annotate(rightarea, xy=(0.15, abs(ymin)/(ymax - ymin) + move[1]), xycoords='axes fraction', bbox=dict(boxstyle=\"round\", fc=\"w\"), size = 12)\n",
    "    \n",
    "    elif ypio == 1:\n",
    "        axr.fill_betweenx(y, 0, dy, where = y <= 0, alpha = 1, color='#EF7C00')\n",
    "        axr.fill_betweenx(y, 0, dy, where = y >= 0, alpha = 1, color='#003D7C')\n",
    "\n",
    "        leftarea = np.round( sum(n <= 0 for n in all_y)/len(all_y),2 )\n",
    "        rightarea = np.round( sum(n > 0 for n in all_y)/len(all_y),2 )\n",
    "\n",
    "        axr.annotate(leftarea, xy=(0.15, abs(ymin)/(ymax - ymin) + move[0]), xycoords='axes fraction', bbox=dict(boxstyle=\"round\", fc=\"w\"), size = 12)\n",
    "        axr.annotate(rightarea, xy=(0.15, abs(ymin)/(ymax - ymin) + move[1]), xycoords='axes fraction', bbox=dict(boxstyle=\"round\", fc=\"w\"), size = 12)\n",
    "    else:\n",
    "        axr.fill_betweenx(y, 0, dy, where = y <= 100, alpha = 1, color='#EF7C00') #pio\n",
    "        axr.fill_betweenx(y, 0, dy, where = y >= 100, alpha = 1, color='#003D7C') #pio\n",
    "\n",
    "        leftarea = np.round( sum(n <= 100 for n in all_y)/len(all_y),2 ) #pio\n",
    "        rightarea = np.round( sum(n > 100 for n in all_y)/len(all_y),2 ) #pio\n",
    "\n",
    "        axr.annotate(leftarea, xy=(0.15, (100 - ymin)/(ymax - ymin) + move[0]), xycoords='axes fraction', bbox=dict(boxstyle=\"round\", fc=\"w\"), size = 12)\n",
    "        axr.annotate(rightarea, xy=(0.15, (100 - ymin)/(ymax - ymin) + move[1]), xycoords='axes fraction', bbox=dict(boxstyle=\"round\", fc=\"w\"), size = 12)\n",
    "#     axr.invert_xaxis()\n",
    "\n",
    "    ####### Create X-marginal (top)\n",
    "    axt = plt.subplot(gs[0,0], frameon = False, yticks = [], xticks = [])\n",
    "    #base = pyplot.gca().transData\n",
    "    #rot = transforms.Affine2D().rotate_deg(180)\n",
    "    axt.plot(x, dx, color = 'black')\n",
    "    axt.fill_between(x, 0, dx, where = x >= 49.9, alpha= 1, color = '#003D7C')\n",
    "    axt.fill_between(x, 0, dx, where = x <= 50, alpha= 1, color = '#EF7C00')\n",
    "\n",
    "#     axt.invert_yaxis()\n",
    "    leftarea = np.round( sum(n <= 50 for n in all_x)/len(all_x),2 )\n",
    "    rightarea = np.round( sum(n > 50 for n in all_x)/len(all_x),2 )\n",
    "\n",
    "    axt.annotate(leftarea, xy=(move[2], 0.15), xycoords='axes fraction', bbox=dict(boxstyle=\"round\", fc=\"w\"), size = 12)\n",
    "    axt.annotate(rightarea, xy=(move[3], 0.15), xycoords='axes fraction', bbox=dict(boxstyle=\"round\", fc=\"w\"), size = 12)\n",
    "\n",
    "    ####### Bring the marginals closer to the scatter plot and save eps file\n",
    "    fig.tight_layout(pad = 1)\n",
    "    # plt.savefig(figure_name + '.eps', format='eps')\n",
    "    plt.savefig(figure_name + '.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2169bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mu = 0.4\n",
    "# lamb = 0.5\n",
    "# file_name = Result_dir + 'DDR_vs_OLS_mu='+str(mu)+\"_lamb=\"+str(lamb)+\"_post_grid=\"+str(L_N)\n",
    "# all_x = h2h_ddr_vs_ols_post[L_N,mu,lamb] * 100\n",
    "# all_y = regret_rd_ddr_vs_ols_post[L_N,mu,lamb] * 100\n",
    "# figure_plot_upright(all_x, all_y, file_name, size = (5, 5), move = [-0.07, 0.07, 0.35, 0.55], ysame = 0, yrange = [6,6], sublabel = '', ypio = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c1622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "regret_mat = np.zeros((len(mu_all),len(lamb_all)))\n",
    "L_N = 2\n",
    "mu_index = 0\n",
    "for mu in mu_all:\n",
    "    lamb_index = 0\n",
    "    for lamb in lamb_all:\n",
    "        regret_mat[mu_index,lamb_index] = np.round(np.mean(regret_rd_ddr_vs_ols_ante[L_N,mu,lamb]) * 100,2)\n",
    "        lamb_index = lamb_index + 1\n",
    "    mu_index = mu_index + 1\n",
    "regret_df = pd.DataFrame(regret_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aaf586",
   "metadata": {},
   "outputs": [],
   "source": [
    "regret_df.index = mu_all\n",
    "regret_df.columns = lamb_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a51477",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0.4\n",
    "lamb = 0.5\n",
    "file_name = Result_dir + 'DDR_vs_OLS_mu='+str(mu)+\"_lamb=\"+str(lamb)+\"_ante_grid=\"+str(L_N)\n",
    "all_x = h2h_ddr_vs_ols_ante[L_N,mu,lamb] * 100\n",
    "all_y = regret_rd_ddr_vs_ols_ante[L_N,mu,lamb] * 100\n",
    "figure_plot_upright(all_x, all_y, file_name, size = (5, 5), move = [-0.07, 0.07, 0.35, 0.55], ysame = 0, yrange = [6,6], sublabel = '', ypio = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6df7c7",
   "metadata": {},
   "source": [
    "### Impact of Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c662a09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_DDR_vs_OLS_Network_Result(grid_all,mu_all,lamb_all,h2h_ddr_vs_ols_input,regret_rd_ddr_vs_ols_input):\n",
    "    h2h_avg_all = {}; regret_rd_avg_all = {}\n",
    "    h2h_prop_all = {}; regret_rd_prop_all = {}\n",
    "\n",
    "    for grid in grid_all:\n",
    "        L_N = grid[0]\n",
    "        h2h_avg = np.zeros((len(mu_all),len(lamb_all))); regret_rd_avg = np.zeros((len(mu_all),len(lamb_all)))\n",
    "        h2h_prop = np.zeros((len(mu_all),len(lamb_all))); regret_rd_prop = np.zeros((len(mu_all),len(lamb_all)))\n",
    "\n",
    "        mu_index = 0\n",
    "        for mu in mu_all:\n",
    "            lamb_index = 0\n",
    "            for lamb in lamb_all:\n",
    "                h2h_ddr_ols_tem = h2h_ddr_vs_ols_input[L_N,mu,lamb]\n",
    "                regret_rd_ddr_vs_ols_tem = regret_rd_ddr_vs_ols_input[L_N,mu,lamb]\n",
    "\n",
    "                h2h_avg[mu_index,lamb_index] = np.nanmean(h2h_ddr_ols_tem)\n",
    "                regret_rd_avg[mu_index,lamb_index] = np.nanmean(regret_rd_ddr_vs_ols_tem)\n",
    "                \n",
    "                h2h_prop[mu_index,lamb_index] = len(h2h_ddr_ols_tem[h2h_ddr_ols_tem >= 0.5])/len(h2h_ddr_ols_tem)\n",
    "                regret_rd_prop[mu_index,lamb_index] = len(regret_rd_ddr_vs_ols_tem[regret_rd_ddr_vs_ols_tem > 0.0])/len(regret_rd_ddr_vs_ols_tem)\n",
    "        \n",
    "                lamb_index = lamb_index + 1\n",
    "            mu_index = mu_index + 1\n",
    "\n",
    "        h2h_avg_all[L_N] = h2h_avg\n",
    "        regret_rd_avg_all[L_N] = regret_rd_avg\n",
    "        h2h_prop_all[L_N] = h2h_prop\n",
    "        regret_rd_prop_all[L_N] = regret_rd_prop\n",
    "\n",
    "    return h2h_avg_all,regret_rd_avg_all,h2h_prop_all,regret_rd_prop_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6225dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Post Results\n",
    "# h2h_ddr_vs_ols_post_avg_all,regret_rd_ddr_vs_ols_post_avg_all,h2h_ddr_vs_ols_post_prop_all,regret_rd_ddr_vs_ols_post_prop_all = \\\n",
    "#     obtain_DDR_vs_OLS_Network_Result(grid_all,mu_all,lamb_all,h2h_ddr_vs_ols_post,regret_rd_ddr_vs_ols_post)\n",
    "\n",
    "### Ante Results\n",
    "h2h_ddr_vs_ols_ante_avg_all,regret_rd_ddr_vs_ols_ante_avg_all,h2h_ddr_vs_ols_ante_prop_all,regret_rd_ddr_vs_ols_ante_prop_all = \\\n",
    "    obtain_DDR_vs_OLS_Network_Result(grid_all,mu_all,lamb_all,h2h_ddr_vs_ols_ante,regret_rd_ddr_vs_ols_ante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3cc33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "regret_avg_max_post = np.zeros(len(grid_all)); regret_avg_max_indices_post = {}\n",
    "regret_prop_max_post = np.zeros(len(grid_all)); regret_prop_max_indices_post = {}\n",
    "\n",
    "regret_avg_max_ante = np.zeros(len(grid_all)); regret_avg_max_indices_ante = {}\n",
    "regret_prop_max_ante = np.zeros(len(grid_all)); regret_prop_max_indices_ante = {}\n",
    "\n",
    "grid_index = 0\n",
    "for grid in grid_all:\n",
    "    L_N = grid[0]\n",
    "    # ### Post Results\n",
    "    # regret_avg_max_post[grid_index] = np.max(regret_rd_ddr_vs_ols_post_avg_all[L_N]) * 100\n",
    "    # max_index = np.unravel_index(np.argmax(regret_rd_ddr_vs_ols_post_avg_all[L_N]), regret_rd_ddr_vs_ols_post_avg_all[L_N].shape)\n",
    "    # regret_avg_max_indices_post[grid[0]] = (mu_all[max_index[0]],lamb_all[max_index[1]])\n",
    "\n",
    "    # regret_prop_max_post[grid_index] = np.max(regret_rd_ddr_vs_ols_post_prop_all[L_N])\n",
    "    # max_index = np.unravel_index(np.argmax(regret_rd_ddr_vs_ols_post_prop_all[L_N]), regret_rd_ddr_vs_ols_post_prop_all[L_N].shape)\n",
    "    # regret_prop_max_indices_post[grid[0]] = (mu_all[max_index[0]],lamb_all[max_index[1]])\n",
    "\n",
    "\n",
    "    regret_avg_max_ante[grid_index] = np.max(regret_rd_ddr_vs_ols_ante_avg_all[L_N]) * 100\n",
    "    max_index = np.unravel_index(np.argmax(regret_rd_ddr_vs_ols_ante_avg_all[L_N]), regret_rd_ddr_vs_ols_ante_avg_all[L_N].shape)\n",
    "    regret_avg_max_indices_ante[grid[0]] = (mu_all[max_index[0]],lamb_all[max_index[1]])\n",
    "\n",
    "    regret_prop_max_ante[grid_index] = np.max(regret_rd_ddr_vs_ols_ante_prop_all[L_N])\n",
    "    max_index = np.unravel_index(np.argmax(regret_rd_ddr_vs_ols_ante_prop_all[L_N]), regret_rd_ddr_vs_ols_ante_prop_all[L_N].shape)\n",
    "    regret_prop_max_indices_ante[grid[0]] = (mu_all[max_index[0]],lamb_all[max_index[1]])\n",
    "\n",
    "    grid_index = grid_index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8c2efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_max_arr = regret_avg_max_post\n",
    "# prop_max_arr = regret_prop_max_post\n",
    "# avg_opt_para = regret_avg_max_indices_post\n",
    "# prop_opt_para = regret_prop_max_indices_post\n",
    "# # 横坐标只取网格边长\n",
    "# grid_sizes = [g[0] for g in grid_all]\n",
    "# # 绘图\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.plot(grid_sizes, avg_max_arr, marker='o', linestyle='-', color='b', label='Avg Max Value')\n",
    "# plt.plot(grid_sizes, prop_max_arr, marker='o', linestyle='-', color='g', label='Proportion Max Value')\n",
    "# # 添加注释\n",
    "# for i, size in enumerate(grid_sizes):\n",
    "#     x_offset, y_offset = avg_opt_para[size]\n",
    "#     plt.annotate(f\"({x_offset:.1f}, {y_offset:.1f})\",\n",
    "#                  (size, avg_max_arr[i]),\n",
    "#                  textcoords=\"offset points\",\n",
    "#                  xytext=(15, -20),  # 注释位置偏移\n",
    "#                  ha='center',\n",
    "#                  fontsize=10,\n",
    "#                  color = \"blue\")\n",
    "\n",
    "# # 添加注释\n",
    "# for i, size in enumerate(grid_sizes):\n",
    "#     x_offset, y_offset = prop_opt_para[size]\n",
    "#     plt.annotate(f\"({x_offset:.1f}, {y_offset:.1f})\",\n",
    "#                  (size, prop_max_arr[i]),\n",
    "#                  textcoords=\"offset points\",\n",
    "#                  xytext=(15, 10),  # 注释位置偏移\n",
    "#                  ha='center',\n",
    "#                  fontsize=10,\n",
    "#                  color = \"green\")\n",
    "    \n",
    "# # 坐标轴和标题\n",
    "# plt.xlabel(\"Grid Size\",fontsize = 16)\n",
    "# plt.ylabel(\"Max Value\",fontsize = 16)\n",
    "# plt.title(\"Max Value vs Grid Size\")\n",
    "# plt.xticks(grid_sizes,fontsize = 12)\n",
    "# plt.yticks(fontsize = 12)\n",
    "\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.legend()\n",
    "# plt.savefig(Result_dir +\"Impact_Network_Structure_Post.pdf\", format='pdf')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738438fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_max_arr = regret_avg_max_ante\n",
    "prop_max_arr = regret_prop_max_ante\n",
    "avg_opt_para = regret_avg_max_indices_ante\n",
    "prop_opt_para = regret_prop_max_indices_ante\n",
    "# 横坐标只取网格边长\n",
    "grid_sizes = [g[0] for g in grid_all]\n",
    "# 绘图\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(grid_sizes, avg_max_arr, marker='o', linestyle='-', color='b', label='Avg Max Value')\n",
    "plt.plot(grid_sizes, prop_max_arr, marker='o', linestyle='-', color='g', label='Proportion Max Value')\n",
    "# 添加注释\n",
    "for i, size in enumerate(grid_sizes):\n",
    "    x_offset, y_offset = avg_opt_para[size]\n",
    "    plt.annotate(f\"({x_offset:.1f}, {y_offset:.1f})\",\n",
    "                 (size, avg_max_arr[i]),\n",
    "                 textcoords=\"offset points\",\n",
    "                 xytext=(15, -20),  # 注释位置偏移\n",
    "                 ha='center',\n",
    "                 fontsize=10,\n",
    "                 color = \"blue\")\n",
    "\n",
    "# 添加注释\n",
    "for i, size in enumerate(grid_sizes):\n",
    "    x_offset, y_offset = prop_opt_para[size]\n",
    "    plt.annotate(f\"({x_offset:.1f}, {y_offset:.1f})\",\n",
    "                 (size, prop_max_arr[i]),\n",
    "                 textcoords=\"offset points\",\n",
    "                 xytext=(15, 10),  # 注释位置偏移\n",
    "                 ha='center',\n",
    "                 fontsize=10,\n",
    "                 color = \"green\")\n",
    "    \n",
    "# 坐标轴和标题\n",
    "plt.xlabel(\"Grid Size\",fontsize = 16)\n",
    "plt.ylabel(\"Max Value\",fontsize = 16)\n",
    "plt.title(\"Max Value vs Grid Size\")\n",
    "plt.xticks(grid_sizes,fontsize = 12)\n",
    "plt.yticks(fontsize = 12)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig(Result_dir +\"Impact_Network_Structure_Ante.pdf\", format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e12f776",
   "metadata": {},
   "source": [
    "## DDR vs SPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb2e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0.8\n",
    "lamb = 0.3\n",
    "file_name = Result_dir + 'DDR_vs_SPO_mu='+str(mu)+\"_lamb=\"+str(lamb)+\"_ante_grid=\"+str(L_N)\n",
    "all_x = h2h_ddr_vs_ols_ante[L_N,mu,lamb] * 100\n",
    "all_y = regret_rd_ddr_vs_ols_ante[L_N,mu,lamb] * 100\n",
    "figure_plot_upright(all_x, all_y, file_name, size = (5, 5), move = [-0.07, 0.07, 0.35, 0.55], ysame = 0, yrange = [6,6], sublabel = '', ypio = 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
